{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "910aac6a",
   "metadata": {},
   "source": [
    "# Stats: Descriptive Statistics\n",
    "\n",
    "*Purpose*: We will use *descriptive statistics* to make quantitative summaries of a dataset. Descriptive statistics provide a much more compact description than a visualization, and are important when a data consumer wants \"just one number\". However, there are many subtleties to working with descriptive statistics, which we'll discuss in this exercise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e309767",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24de1a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import grama as gr\n",
    "import pandas as pd\n",
    "DF = gr.Intention()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42980c56",
   "metadata": {},
   "source": [
    "# Statistics\n",
    "\n",
    "A *statistic* is a numerical summary of a sample. Statistics are useful because they provide a useful summary about our data. A histogram gives us a rich summary of a datset: for example the departure delay time in the NYC flight data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9564e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: Don't edit; run and inspect\n",
    "from nycflights13 import flights as df_flights\n",
    "\n",
    "(\n",
    "    df_flights\n",
    "    >> gr.ggplot(gr.aes(\"dep_delay\"))\n",
    "    + gr.geom_histogram(bins=60)\n",
    "    + gr.scale_x_log10()\n",
    "    + gr.labs(\n",
    "        x=\"Departure Delay Time (Minutes)\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6564087a",
   "metadata": {},
   "source": [
    "This shows the departure delay time, in minutes.\n",
    "\n",
    "However, we might be interested in a few questions about these data:\n",
    "\n",
    "- What is a *typical* value for the departure delay? (Location)\n",
    "- How *variable* are departure delay times? (Spread)\n",
    "- How much does departure delay *co-vary* with distance? (Dependence)\n",
    "\n",
    "We can give quantitative answers to all these questions using statistics!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7358abfe",
   "metadata": {},
   "source": [
    "## Central Tendency\n",
    "\n",
    "*Central tendency* is the idea of where data tend to be \"located\"---this concept is also called *location*. It is best thought of as the \"center\" of the data. The following graph illustrates central tendency, as operationalized by the *mean* and *median*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59631621",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: No need to edit\n",
    "# Generate data\n",
    "df_standard = gr.df_make(z=gr.marg_mom(\"norm\", mean=0, sd=1).r(1000))\n",
    "\n",
    "# Visualize\n",
    "(\n",
    "    df_standard\n",
    "    >> gr.ggplot(gr.aes(\"z\"))\n",
    "    + gr.geom_density()\n",
    "    + gr.geom_vline(\n",
    "        data=df_standard\n",
    "        >> gr.tf_summarize(\n",
    "            z_mean=gr.mean(DF.z),\n",
    "            z_median=gr.median(DF.z),\n",
    "        )\n",
    "        >> gr.tf_pivot_longer(\n",
    "            columns=[\"z_mean\", \"z_median\"],\n",
    "            names_to=[\".value\", \"stat\"],\n",
    "            names_sep=\"_\",\n",
    "        ),\n",
    "        mapping=gr.aes(xintercept=\"z\", color=\"stat\"),\n",
    "        size=1,\n",
    "    )\n",
    "    + gr.scale_color_discrete(name=\"Statistic\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4d6cb4",
   "metadata": {},
   "source": [
    "There are two primary measures of central tendency; the *mean* and *median*. The mean is the simple [arithmetic average](https://en.wikipedia.org/wiki/Arithmetic_mean): the sum of all values divided by the total number of values. The mean is denoted by $\\overline{x}$ and defined by\n",
    "\n",
    "$$\\overline{X} = \\frac{1}{n} \\sum_{i=1}^n X_i,$$\n",
    "\n",
    "where $n$ is the number of data points, and the $X_i$ are the individual values.\n",
    "\n",
    "The [median](https://en.wikipedia.org/wiki/Median) is the value that separates half the data above and below. Weirdly, there's no standard symbol for the median, so we'll just denote it as $\\text{Median}[X]$ to denote the median of a quantity $X$.\n",
    "\n",
    "The median is a *robust* statistic, which is best illustrated by example. Consider the following two samples `v_base` and `v_outlier`. The sample `v_outlier` has an *outlier*, a value very different from the other values. Observe what value the mean and median take for these different samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d84fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: No need to change this!\n",
    "v_base = pd.Series([1, 2, 3, 4, 5])\n",
    "v_outlier = v_base.copy()\n",
    "v_outlier[-1] = 1e3\n",
    "\n",
    "gr.df_make(\n",
    "  mean_base=gr.mean(v_base),\n",
    "  median_base=gr.median(v_base),\n",
    "\n",
    "  mean_outlier=gr.mean(v_outlier),\n",
    "  median_outlier=gr.median(v_outlier)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7f868c",
   "metadata": {},
   "source": [
    "Note that for `v_outlier` the mean is greatly increased, but the median is only slightly changed. It is in this sense that the median is *robust*---it is robust to outliers. When one has a dataset with outliers, the median is usually a better measure of central tendency [1].\n",
    "\n",
    "It can be useful to recognize when the mean and median agree or disagree with each other. For instance, with the flights data we see strong disagreement between the `mean` and `median` of `dep_delay`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebef524",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_flights\n",
    "    >> gr.tf_filter(DF.dep_delay < 60)\n",
    "    >> gr.ggplot(gr.aes(\"dep_delay\"))\n",
    "    + gr.geom_histogram(bins=60)\n",
    "    + gr.geom_vline(\n",
    "        data=df_flights\n",
    "        >> gr.tf_summarize(\n",
    "            dep_delayXmean=gr.mean(DF.dep_delay),\n",
    "            dep_delayXmedian=gr.median(DF.dep_delay),\n",
    "        )\n",
    "        >> gr.tf_pivot_longer(\n",
    "            columns=[\"dep_delayXmean\", \"dep_delayXmedian\"],\n",
    "            names_to=[\".value\", \"stat\"],\n",
    "            names_sep=\"X\",\n",
    "        ),\n",
    "        mapping=gr.aes(xintercept=\"dep_delay\", color=\"stat\"),\n",
    "    )\n",
    "    + gr.scale_color_discrete(name=\"Statistic\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ec8adf",
   "metadata": {},
   "source": [
    "The median is in the bulk of the negative values, while the mean is pulled higher by the long right tail of the distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2ee15f",
   "metadata": {},
   "source": [
    "### __q1__ Compare conclusions from mean and median\n",
    "\n",
    "The following code computes the mean and median `dep_delay` for each carrier, and sorts based on mean. Duplicate the code, and sort by median instead. Answer the questions under *observations* below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70daf54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: No need to edit; adapt this code to sort by median\n",
    "(\n",
    "    df_flights\n",
    "    >> gr.tf_group_by(DF.carrier)\n",
    "    >> gr.tf_summarize(\n",
    "        mean=gr.mean(DF.dep_delay),\n",
    "        median=gr.median(DF.dep_delay),\n",
    "    )\n",
    "    >> gr.tf_ungroup()\n",
    "    >> gr.tf_arrange(gr.desc(DF[\"mean\"]))\n",
    "    >> gr.tf_head(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c46cfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: No need to edit; adapt this code to sort by median\n",
    "(\n",
    "    df_flights\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dcbf6d",
   "metadata": {},
   "source": [
    "*Observations*\n",
    "\n",
    "- Which carriers show up in the top 5 in both lists?\n",
    "  - (Your response here)\n",
    "- Which carriers show up in the top 5 for only *one* of the lists?\n",
    "  - (Your response here)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8b561c",
   "metadata": {},
   "source": [
    "## Aside: Some variable names are tricky...\n",
    "\n",
    "There's an issue we'll start to run into as we start naming columns the same as builtin functions. For instance, the following code will throw an error:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7eca7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Uncomment the code below and run\n",
    "## NOTE: This will throw a huge, scary error!\n",
    "# (\n",
    "#     df_flights\n",
    "#     >> gr.tf_summarize(\n",
    "#         ## Here we assign a new column with the name `mean`\n",
    "#         mean=gr.mean(DF.dep_delay)\n",
    "#     )\n",
    "#     >> gr.tf_mutate(\n",
    "#         ## Here we attempt to access that `mean` column, but it goes wrong....\n",
    "#         mean2x=DF.mean * 2\n",
    "#     )\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8d69d1",
   "metadata": {},
   "source": [
    "The issue here is a bit technical, but it boils down to the fact that `mean` is now a column of the DataFrame, but also a *method* that we can call as a function `df.mean()`.\n",
    "\n",
    "There's a way around this issue, which is to access the column through bracket notation `df[\"column\"]` rather than dot notation `df.column`. This will correctly access the column `mean` rather than the method `mean()`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e09908f",
   "metadata": {},
   "source": [
    "### __q2__ Fix this code\n",
    "\n",
    "Use bracket `[]` notation to access the `\"mean\"` column and fix the code below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7e2d04",
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "## TASK: Use bracket [] notation to fix the following code\n",
    "(\n",
    "    df_flights\n",
    "    >> gr.tf_summarize(mean=gr.mean(DF.dep_delay))\n",
    "    >> gr.tf_mutate(\n",
    "        # mean2x=DF.mean * 2 # TODO: Uncomment and fix this code\n",
    "\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865019eb",
   "metadata": {},
   "source": [
    "## Multi-modality\n",
    "\n",
    "It may not seem like it, but we're actually *making an assumption* when we use the mean (or median) as a typical value. Imagine we had the following data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b902280",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: No need to edit\n",
    "df_bimodal = (\n",
    "    gr.df_make(x=gr.marg_mom(\"norm\", mean=-2, sd=1).r(300))\n",
    "    >> gr.tf_bind_rows(gr.df_make(x=gr.marg_mom(\"norm\", mean=+2, sd=1).r(300)))\n",
    ")\n",
    "\n",
    "(\n",
    "    df_bimodal\n",
    "    >> gr.ggplot(gr.aes(\"x\"))\n",
    "    + gr.geom_histogram(bins=30)\n",
    "    + gr.geom_vline(\n",
    "        data=df_bimodal\n",
    "        >> gr.tf_summarize(\n",
    "            x_mean=gr.mean(DF.x),\n",
    "            x_median=gr.median(DF.x),\n",
    "        )\n",
    "        >> gr.tf_pivot_longer(\n",
    "            columns=[\"x_mean\", \"x_median\"],\n",
    "            names_to=[\".value\", \"stat\"],\n",
    "            names_sep=\"_\",\n",
    "        ),\n",
    "        mapping=gr.aes(xintercept=\"x\", color=\"stat\"),\n",
    "    )\n",
    "    + gr.scale_color_discrete(name=\"Statistic\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e1aa13",
   "metadata": {},
   "source": [
    "Here the mean and median are both close to zero, but *zero is an atypical number*! This is partly why we don't *only* compute descriptive statistics, but also do a deeper dive into our data. Here, we should probably refuse to give a single typical value; instead, it seems there might really be two populations showing up in the same dataset, so we can give two typical numbers, say `-2, +2`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981ec569",
   "metadata": {},
   "source": [
    "## Quantiles\n",
    "\n",
    "Before we can talk about spread, we need to talk about *quantiles*. A [quantile](https://en.wikipedia.org/wiki/Quantile) is a value that separates a user-specified fraction of data (or a distribution). For instance, the median is the $50\\%$ quantile; thus $\\text{Median}[X] = Q_{0.5}[X]$. We can generalize this idea to talk about any quantile between $0\\%$ and $100\\%$.\n",
    "\n",
    "The following graph visualizes the $25\\%, 50\\%, 75\\%$ quantiles of a standard normal. Since these are the quarter-quantiles ($1/4, 2/4, 3/4$), these are often called the *quartiles*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef730f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: No need to edit\n",
    "mg_standard = gr.marg_mom(\"norm\", mean=0, sd=1)\n",
    "\n",
    "(\n",
    "    gr.df_make(z=gr.linspace(-3, +3, 500))\n",
    "    >> gr.tf_mutate(d=mg_standard.d(DF.z))\n",
    "    \n",
    "    >> gr.ggplot(gr.aes(\"z\", \"d\"))\n",
    "    + gr.geom_line()\n",
    "    + gr.geom_segment(\n",
    "        data=gr.df_make(p=[0.25, 0.50, 0.75])\n",
    "        >> gr.tf_mutate(z=mg_standard.q(DF.p))\n",
    "        >> gr.tf_mutate(d=mg_standard.d(DF.z)),\n",
    "        mapping=gr.aes(xend=\"z\", yend=0, color=\"factor(p)\")\n",
    "    )\n",
    "    + gr.scale_color_discrete(name=\"Quantiles\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263e1613",
   "metadata": {},
   "source": [
    "*Note*: This code makes use of *distributions* such as `mg_standard`; we'll learn more about that in a future exercise. For this exercise, we're focused more on taking statistics of datasets.\n",
    "\n",
    "We'll use the quartiles to define the *interquartile range*. First, the `quant()` function computes quantiles of a sample. For example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0109fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: No need to change this! Run for an example\n",
    "(\n",
    "    df_flights\n",
    "    >> gr.tf_summarize(\n",
    "        dep_delay_25=gr.quant(DF.dep_delay, 0.25),\n",
    "        dep_delay_50=gr.quant(DF.dep_delay, 0.50),\n",
    "        dep_delay_75=gr.quant(DF.dep_delay, 0.75),\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a622ef3",
   "metadata": {},
   "source": [
    "We provide to `gr.quant()` a column to analyze and a probability level. Remember: a *probability* is a value between $[0, 1]$, while a *quantile* is a value that probably has units, like minutes in the case of `dep_delay`.\n",
    "\n",
    "Now we can define the interquartile range:\n",
    "\n",
    "$$IQR[X] = Q_{0.75}[X] - Q_{0.25}[X],$$\n",
    "\n",
    "where $Q_{p}[X]$ is the $p$-th quantile of a sample $X$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51df53b",
   "metadata": {},
   "source": [
    "### __q3__ Compute the IQR\n",
    "\n",
    "Using the function `quantile`, compute the *interquartile range*; this is the difference between the $75%$ and $25%$ quantiles. Provide this as a column called `iqr`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6d4155",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TASK: Compute the IQR\n",
    "df_standard_iqr = (\n",
    "    df_standard\n",
    "    >> gr.tf_summarize(\n",
    "        ## TODO: Compute the IQR, provide as the column `iqr`\n",
    "\n",
    "    )\n",
    ")\n",
    "\n",
    "## NOTE: Use this to check your work\n",
    "print(df_standard_iqr)\n",
    "\n",
    "assert \\\n",
    "    \"iqr\" in df_standard_iqr.columns, \\\n",
    "    \"df_standard_iqr does not have an `iqr` column\"\n",
    "assert \\\n",
    "    abs(df_standard_iqr.iqr.values[0] - gr.IQR(df_standard.z)) < 1e-3, \\\n",
    "    \"Computed IQR is incorrect\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d30bd74",
   "metadata": {},
   "source": [
    "## Spread\n",
    "\n",
    "*Spread* is the concept of how tightly or widely data are *spread out*. There are two primary measures of spread: the *standard deviation*, and the *interquartile range*.\n",
    "\n",
    "The [standard deviation](https://en.wikipedia.org/wiki/Standard_deviation) (SD) is denoted by $s$ and defined by\n",
    "\n",
    "$$s = \\sqrt{ \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\overline{X})^2 },$$\n",
    "\n",
    "where $\\overline{X}$ is the mean of the data. Note the factor of $n-1$ rather than $n$: This slippery idea is called [Bessel's correction](https://en.wikipedia.org/wiki/Bessel%27s_correction). Note that $\\sigma^2$ is called the *variance*.\n",
    "\n",
    "By way of analogy, mean is to standard deviation as median is to IQR: The IQR is a robust measure of spread. Returning to our outlier example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5894ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: No need to change this!\n",
    "v_base = pd.Series([1, 2, 3, 4, 5])\n",
    "v_outlier = v_base.copy()\n",
    "v_outlier[-1] = 1e3\n",
    "\n",
    "gr.df_make(\n",
    "  sd_base=gr.sd(v_base),\n",
    "  IQR_base=gr.IQR(v_base),\n",
    "\n",
    "  sd_outlier=gr.sd(v_outlier),\n",
    "  IQR_outlier=gr.IQR(v_outlier)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a5043a",
   "metadata": {},
   "source": [
    "### __q4__ Compare conclusions from sd and IQR\n",
    "\n",
    "Using the code from q2 as a starting point, compute the standard deviation (`sd()`) and interquartile range (`IQR()`), and rank the top five carriers, this time by sd and IQR. Answer the questions under *observations* below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ccc5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TASK: Find top 5 sd and IQR carriers\n",
    "print(\n",
    "    df_flights\n",
    "    ## TODO: Compute sd and IQR of dep_delay, take top 5 highest sd\n",
    "\n",
    ")\n",
    "\n",
    "print(\n",
    "    df_flights\n",
    "    ## TODO: Compute sd and IQR of dep_delay, take top 5 highest IQR\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2073b0f0",
   "metadata": {},
   "source": [
    "*Observations*\n",
    "\n",
    "- Which carriers show up in the top 5 in both lists?\n",
    "  - (Your response here)\n",
    "- Which carriers show up in the top 5 for only *one* of the lists?\n",
    "  - (Your response here)\n",
    "- Which carrier (among your top-ranked) has the largest difference between `sd` and `IQR`? What might this difference suggest?\n",
    "  - (Your response here)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b63bf9",
   "metadata": {},
   "source": [
    "## Dependence\n",
    "\n",
    "So far, we've talked about descriptive statistics to consider one variable at a time. To conclude, we'll talk about statistics to consider *dependence* between two variables in a dataset.\n",
    "\n",
    "[Dependence](https://en.wikipedia.org/wiki/Correlation_and_dependence)---like location or spread---is a general idea of relation between two variables. For instance, when it comes to flights we'd expect trips between more distant airports to take longer. If we plot `distance` vs `air_time` in a scatterplot, we indeed see this dependence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab163ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: No need to edit\n",
    "(\n",
    "    df_flights\n",
    "    >> gr.tf_sample(frac=0.10) # Subsample the rows for speed\n",
    "    \n",
    "    >> gr.ggplot(gr.aes(\"air_time\", \"distance\"))\n",
    "    + gr.geom_point()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5d8d94",
   "metadata": {},
   "source": [
    "Two flavors of correlation help us make this idea quantitative: the *Pearson correlation* and *Spearman correlation*. Unlike our previous quantities for location and spread, these correlations are *dimensionless* (they have no units), and they are bounded between $[-1, +1]$.\n",
    "\n",
    "The [Pearson correlation](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) is often denoted by $r_{XY}$, and it specifies the variables being considered $X, Y$. It is defined by\n",
    "\n",
    "$$r_{XY} = \\frac{\\sum_{i=1}^n (X_i - \\overline{X}) (Y_i - \\overline{Y})}{s_X s_Y}.$$\n",
    "\n",
    "The [Spearman correlation](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient) is often denoted by $\\rho_{XY}$, and is actually defined in terms of the Pearson correlation $r_{XY}$, but with the ranks ($1$ to $n$) rather than the values $X_i, Y_i$.\n",
    "\n",
    "For example, we might expect a strong correlation between the `air_time` and the `distance` between airports. The function `gr.corr()` can compute both Pearson and Spearman correlation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421516b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: No need to edit\n",
    "(\n",
    "    df_flights\n",
    "    >> gr.tf_summarize(\n",
    "        r=gr.corr(DF.air_time, DF.distance, nan_drop=True), # Default is pearson correlation\n",
    "        rho=gr.corr(DF.air_time, DF.distance, method=\"spearman\", nan_drop=True),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51eabe7f",
   "metadata": {},
   "source": [
    "As expected, we see a strong correlation between `air_time` and `distance` (according to both correlation metrics).\n",
    "\n",
    "However, we wouldn't expect any relation between `air_time` and `month`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24bfb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: No need to edit\n",
    "(\n",
    "    df_flights\n",
    "    >> gr.tf_summarize(\n",
    "        r=gr.corr(DF.air_time, DF.month, nan_drop=True), # Default is pearson correlation\n",
    "        rho=gr.corr(DF.air_time, DF.month, method=\"spearman\", nan_drop=True),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e271b8",
   "metadata": {},
   "source": [
    "In the case of a *perfect linear relationships* the Pearson correlation takes the value $+1$ (for a positive slope) or $-1$ for a negative slope.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a586fd2",
   "metadata": {},
   "source": [
    "### __q5__ Build your intuition for correlation\n",
    "\n",
    "Compute the Pearson correlation between `x, y` below. Play with the `slope` and observe the change in the correlation. Answer the questions under *observations* below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b0b9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TASK: Vary the slope and re-run the code below\n",
    "slope = 0.5\n",
    "\n",
    "## NOTE: No need to edit beyond here\n",
    "df_line = (\n",
    "    gr.df_make(x=gr.linspace(-1, +1, 50))\n",
    "    >> gr.tf_mutate(y=slope * DF.x)\n",
    ")\n",
    "\n",
    "print(\n",
    "    df_line\n",
    "    >> gr.tf_summarize(r=gr.corr(DF.x, DF.y))\n",
    ")\n",
    "\n",
    "(\n",
    "    df_line\n",
    "    >> gr.ggplot(gr.aes(\"x\", \"y\"))\n",
    "    + gr.geom_point()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff977cb0",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    "\n",
    "- For what values of `slope` is the correlation positive?\n",
    "  - (Your response here)\n",
    "- For what values of `slope` is the correlation negative?\n",
    "  - (Your response here)\n",
    "- Is correlation a measure of the slope?\n",
    "  - (Your response here)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfa8e22",
   "metadata": {},
   "source": [
    "Note that this means *correlation is a measure of dependence*; it is **not** a measure of slope! It is better thought of as how *strong* the relationship between two variables is. A closer-to-zero correlation indicates a noisy relationship between variables, while a closer-to-one (in absolute value) indicates a more perfect, predictable relationship between the variables. For instance, the following code simulates data with different correlations, and facets the data based on the underlying correlation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b109fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: No need to edit\n",
    "# Generate data\n",
    "df_correlated = gr.df_grid()\n",
    "v_corr = [-1.0, -0.75, -0.5, -0.25, +0.0, +0.25, +0.5, +0.75, +1.0]\n",
    "n = df_standard.shape[0]\n",
    "for r in v_corr:\n",
    "    df_correlated = (\n",
    "        df_correlated\n",
    "        >> gr.tf_bind_rows(\n",
    "            df_standard\n",
    "            >> gr.tf_mutate(\n",
    "                # Use the conditional gaussian formula to generate correlated observations\n",
    "                w=r * DF.z + mg_standard.r(n) * gr.sqrt(1 - r**2),\n",
    "                r=r,\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "# Visualize\n",
    "(\n",
    "    df_correlated\n",
    "    >> gr.ggplot(gr.aes(\"z\", \"w\"))\n",
    "    + gr.geom_point()\n",
    "    + gr.facet_wrap(\"r\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e167f13",
   "metadata": {},
   "source": [
    "One of the primary differences between Pearson and Spearman is that Pearson is a *linear correlation*, while Spearman is a *nonlinear correlation*. \n",
    "\n",
    "For instance, the following data have a perfect *relationship*. However, the pearson correlation suggests the relationship is imperfect, while the spearman correlation correctly indicates a perfect relationship.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf7f19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: No need to edit\n",
    "df_monotone = (\n",
    "    gr.df_make(x=gr.linspace(-3.14159/2 + 0.1, +3.14159/2 - 0.1, 50))\n",
    "    >> gr.tf_mutate(y=gr.tan(DF.x))\n",
    ")\n",
    "\n",
    "print(\n",
    "    df_monotone\n",
    "    >> gr.tf_summarize(\n",
    "        r=gr.corr(DF.x, DF.y),\n",
    "        rho=gr.corr(DF.x, DF.y, method=\"spearman\")\n",
    "    )\n",
    ")\n",
    "\n",
    "(\n",
    "    df_monotone\n",
    "    >> gr.ggplot(gr.aes(\"x\", \"y\"))\n",
    "    + gr.geom_point()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b375e287",
   "metadata": {},
   "source": [
    "One more note about functional relationships: Neither Pearson nor Spearman can pick up on arbitrary dependencies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea91500",
   "metadata": {},
   "source": [
    "### __q6__ Make a prediction\n",
    "\n",
    "Run the code chunk below and look at the visualization: Make a prediction about what you think the correlation will be. Then compute the Pearson correlation between `x, y` below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758fddd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: Don't edit; run and inspect\n",
    "df_quad = (\n",
    "    gr.df_make(x=gr.linspace(-1, +1, 51))\n",
    "    >> gr.tf_mutate(y=DF.x**2 - 0.5)\n",
    ")\n",
    "\n",
    "(\n",
    "    df_quad\n",
    "    >> gr.ggplot(gr.aes(\"x\", \"y\"))\n",
    "    + gr.geom_point()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fda60cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TASK: Compute the correlation between `x` and `y`\n",
    "(\n",
    "    df_quad\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcb33f9",
   "metadata": {},
   "source": [
    "*Observations*\n",
    "\n",
    "- What do you predict the correlations to be?\n",
    "  - (Your response here)\n",
    "- What are the actual correlations?\n",
    "  - (Your response here)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d516984",
   "metadata": {},
   "source": [
    "One last point about correlation: The mean is to Pearson correlation as the median is to Spearman correlation. The median and Spearman's rho are robust to outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b019d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: No need to edit\n",
    "df_corr_outliers = (\n",
    "    gr.df_make(x=mg_standard.r(25))\n",
    "    >> gr.tf_mutate(y=0.9 * DF.x + gr.sqrt(1-0.9**2) * mg_standard.r(25))\n",
    "    >> gr.tf_bind_rows(gr.df_make(\n",
    "        x=[-10.1, -10, 10, 10.1],\n",
    "        y=[-1.2, -1.1, 1.1, 1.2],\n",
    "    ))\n",
    ")\n",
    "\n",
    "print(\n",
    "    df_corr_outliers\n",
    "    >> gr.tf_summarize(\n",
    "        r=gr.corr(DF.x, DF.y),\n",
    "        rho=gr.corr(DF.x, DF.y, method=\"spearman\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "(\n",
    "    df_corr_outliers\n",
    "    >> gr.ggplot(gr.aes(\"x\", \"y\"))\n",
    "    + gr.geom_point()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba421dde",
   "metadata": {},
   "source": [
    "# Real Variability vs Error\n",
    "\n",
    "As we've seen in this exercise, we are making **modeling assumptions** even when we do simple things like compute and use the mean. We should use multiple analyses---such as multiple statistics and visual inspection---to fully make sense of a dataset.\n",
    "\n",
    "However, beyond simply treating the data as a set of numbers, we should also think critically about what the data *mean*, and what the sources of variability might be. Recall that sources of variability can be either [real or erroneous](https://zdelrosario.github.io/evc-course/exercises_solution/d08-e-stat02-source-solution.html); that is, the source either affect the physical system we are studying, or it can represent corrupted measurements. Most likely, the variability we see is a combination of both real and erroneous sources.\n",
    "\n",
    "To close out this exercise, let's practice reasoning about sources of variability and choosing appropriate statistics for our analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9b7391",
   "metadata": {},
   "source": [
    "### __q7__ Reason about sources of variability\n",
    "\n",
    "Choose a set of statistics to compute for the `dep_delay` column. Answer the questions under *observations* below. Note that you will need to compute multiple statistics *in response* to the questions below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbae8bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TASK: Compute your statistics on `dep_delay`\n",
    "(\n",
    "    df_flights\n",
    "    >> gr.tf_summarize(\n",
    "\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a0086a",
   "metadata": {},
   "source": [
    "*Observations*\n",
    "\n",
    "- Is the variability in `dep_delay` most likely real or erroneous?\n",
    "  - (Your response here)\n",
    "- Compute the mean of `dep_delay`. Does this summary *on its own* suggest that departures could occur early?\n",
    "  - (Your response here)\n",
    "- Is it ever the case that a plane departs early? How typical is that occurrence?\n",
    "  - (Your response here)\n",
    "- Compute the mean of `dep_delay`. Does this summary *on its own*\n",
    "  - (Your response here)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b97890",
   "metadata": {},
   "source": [
    "### __q8__ Select the most consistent airline\n",
    "\n",
    "Take a look at the questions under *observations* below; compute the statistics necessary to answer the following questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446204c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TASK: Compute your statistics on `dep_delay`\n",
    "(\n",
    "    df_flights\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ff98eb",
   "metadata": {},
   "source": [
    "*Observations*\n",
    "\n",
    "- Which `carrier` is the most consistent, in terms of `dep_delay`?\n",
    "  - (Your response here)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
